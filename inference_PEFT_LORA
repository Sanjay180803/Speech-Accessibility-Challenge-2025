import os
import argparse
from tqdm import tqdm
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from peft import PeftModel
import torch
import pandas as pd
import librosa

# Set up argument parsing
parser = argparse.ArgumentParser(description="Inference Script for Whisper Model")
parser.add_argument("--metadata_path", type=str, required=True, help="Path to the metadata file (.csv or .tsv)")
parser.add_argument("--model_path", type=str, required=True, help="Path to the base Whisper model")
parser.add_argument("--adapter_path", type=str, required=True, help="Path to the LoRA adapter")
parser.add_argument("--output_path", type=str, required=True, help="Path to save the .hypo file")

args = parser.parse_args()

# Use the parsed arguments
metadata_path = args.metadata_path
model_path = args.model_path
adapter_path = args.adapter_path
output_path = args.output_path

# Load the processor and model
processor = WhisperProcessor.from_pretrained(model_path)

# Load the base model and LoRA adapter
model = WhisperForConditionalGeneration.from_pretrained(model_path)
model = PeftModel.from_pretrained(model, adapter_path)

# Load metadata file
if metadata_path.endswith(".tsv"):
    dataset = pd.read_csv(metadata_path, sep="\t", header=None,  skiprows=1)
elif metadata_path.endswith(".csv"):
    dataset = pd.read_csv(metadata_path,  skiprows=1)
else:
    raise ValueError("Unsupported metadata file format. Use a .csv or .tsv file.")

# Ensure the necessary columns exist
if dataset.shape[1] < 1:
    raise ValueError("Metadata file must contain at least one column with audio file paths!")

# Open output file for writing predictions
with open(output_path, "w") as fhypo:
    # Skip the header and process each line in the manifest
    for _, row in tqdm(dataset.iterrows(), total=len(dataset)):
        audio_file = row[0]  # First column contains the audio file path

        # Load and preprocess the audio file
        try:
            audio, sr = librosa.load(audio_file, sr=16000)  # Ensure 16 kHz sampling rate
            audio_input = processor(audio, sampling_rate=16000, return_tensors="pt").input_features

            # Perform inference
            model.eval()
            with torch.no_grad():
                result = model.generate(audio_input, language="en")
                predicted_text = processor.batch_decode(result, skip_special_tokens=True)[0]

            # Write transcription to file
            print(predicted_text.strip(), file=fhypo)

            # Optional: Print for debug
            print(f"Predicted Text for {audio_file}: {predicted_text}")
        except Exception as e:
            print(f"Error processing file {audio_file}: {e}")
